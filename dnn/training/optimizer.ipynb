{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Daichi Yoshikawa <daichi.yoshikawa@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base class for optimizers.\n",
    "\n",
    "    Warning: This class should not be used directly.\n",
    "    Use derived classes instead.\n",
    "    \"\"\"\n",
    "    Type = Enum(\n",
    "            'Type',\n",
    "            'sgd, momentum, ada_grad, adam, ada_delta, rms_prop, smorms3'\n",
    "    )\n",
    "\n",
    "    def get_type(self):\n",
    "        raise NotImplementedError('Optimizer.get_type')\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        raise NotImplementedError('Optimizer.update')\n",
    "\n",
    "    def regularization(self, learning_rate, weight_decay):\n",
    "        \"\"\"\n",
    "        Returns value which is supposed to multiplied to weights of\n",
    "        neural network to keep them small values as possible.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        learning_rate : float\n",
    "            Learning rate. Generally > 0.0 and <= 0.3.\n",
    "        weight_decay : float\n",
    "            Degree of weight decay. Generally >= 0.0 and <= 0.3.\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Degree of regularization.\n",
    "        \"\"\"\n",
    "\n",
    "        return (1. - learning_rate * weight_decay)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 3e-2)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'sgd'\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "        w -= self.learning_rate * dw\n",
    "\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 3e-2)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.momentum_rate = kwargs.pop('momentum_rate', 0.9)\n",
    "        self.pre_dw = None\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'momentum'\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.pre_dw is None:\n",
    "            self.pre_dw = np.zeros_like(dw)\n",
    "\n",
    "        self.pre_dw = self.learning_rate*dw + self.momentum_rate*self.pre_dw\n",
    "        w -= self.pre_dw\n",
    "\n",
    "\n",
    "class AdaGrad(Optimizer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    Adaptive Subgradient Methods for\n",
    "    Online Learning and Stochastic Optimization\n",
    "    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 3e-2)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.ep = kwargs.pop('ep', 1e-5)\n",
    "        self.h = None\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'ada_grad'\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.h is None:\n",
    "            self.h = np.zeros_like(w)\n",
    "\n",
    "        self.h += np.power(dw, 2)\n",
    "        dw *= self.dtype(1.) / np.sqrt(self.h + self.ep)\n",
    "        w -= self.learning_rate * dw\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    Adam: A Method for Stochastic Optimization\n",
    "    https://arxiv.org/pdf/1412.6980v9.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 1e-3)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.beta = kwargs.pop('beta', 0.9)\n",
    "        self.gamma = kwargs.pop('gamma', 0.999)\n",
    "        self.ep = kwargs.pop('ep', 1e-5)\n",
    "        self.v = None\n",
    "        self.r = None\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'adam'\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(w)\n",
    "\n",
    "        if self.r is None:\n",
    "            self.r = np.zeros_like(w)\n",
    "\n",
    "        dw_square = np.power(dw, 2)\n",
    "        self.v = self.beta * (self.v - dw) + dw\n",
    "        self.r = self.gamma * (self.r - dw_square) + dw_square\n",
    "\n",
    "        dw = self.learning_rate / np.sqrt(self.r + self.ep) * self.v\n",
    "        w -= dw\n",
    "\n",
    "\n",
    "class AdaDelta(Optimizer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ADADELTA: AN ADAPTIVE LEARNING RATE METHOD\n",
    "    http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 1e-3)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.gamma = kwargs.pop('gamma', 0.95)\n",
    "        self.ep = kwargs.pop('ep', 1e-5)\n",
    "        self.r = None\n",
    "        self.s = None\n",
    "        self.v = None\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.r is None:\n",
    "            self.r = np.zeros_like(w)\n",
    "        if self.s is None:\n",
    "            self.s = np.zeros_like(w)\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(w)\n",
    "\n",
    "        self.r = self.gamma * self.r + (1. - self.gamma) * np.power(dw, 2)\n",
    "        self.v = np.sqrt(self.s + self.ep) / (np.sqrt(self.r + self.ep)) * dw\n",
    "        w -= self.learning_rate * self.v\n",
    "        self.s = self.gamma + (1. - self.gamma) * np.power(self.v, 2)\n",
    "\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    Neural Networks for Machine Learning\n",
    "    Lecture 6a\n",
    "    Overview of mini-batch gradient descent\n",
    "    http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 1e-3)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.gamma = kwargs.pop('gamma', 0.9)\n",
    "        self.ep = kwargs.pop('ep', 1e-5)\n",
    "        self.h = None\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.h is None:\n",
    "            self.h = np.zeros_like(w)\n",
    "\n",
    "        self.h = self.gamma * self.h + (1. - self.gamma) * np.power(dw, 2)\n",
    "        dw *= 1. / (np.sqrt(self.h) + self.ep)\n",
    "        w -= self.learning_rate * dw\n",
    "\n",
    "\n",
    "class SMORMS3(Optimizer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    Rmsprop loses to smorms3 - beware the epsilon! \n",
    "     http://sifter.org/Ëœsimon/journal/20150420.html\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32, **kwargs):\n",
    "        self.dtype = dtype\n",
    "        self.learning_rate = kwargs.pop('learning_rate', 1e-3)\n",
    "        self.weight_decay = kwargs.pop('weight_decay', 0.)\n",
    "        self.ep = kwargs.pop('ep', 1e-5)\n",
    "        self.s = 1.\n",
    "        self.v = None\n",
    "        self.r = None\n",
    "        self.x = None\n",
    "\n",
    "    def optimize(self, w, dw):\n",
    "        w[1:, :] *= self.regularization(self.learning_rate, self.weight_decay)\n",
    "\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(w)\n",
    "        if self.r is None:\n",
    "            self.r = np.zeros_like(w)\n",
    "        if self.x is None:\n",
    "            self.x = np.zeros_like(w)\n",
    "\n",
    "        beta = 1. / self.s\n",
    "\n",
    "        self.v = beta * self.v + (1. - beta) * dw\n",
    "        self.r = beta * self.r + (1. - beta) * np.power(dw, 2)\n",
    "        self.x = np.power(self.v, 2) / (self.r + self.ep)\n",
    "\n",
    "        dw *= np.minimum(self.x, self.learning_rate)\n",
    "        dw /= np.sqrt(self.r) + self.ep\n",
    "        w -= dw\n",
    "        self.s = 1. + (1. - self.x) * self.s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
