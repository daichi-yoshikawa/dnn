{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-14631abaea43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_kwarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomWeight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackPropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daichi/Workspace/oss/python3_ws/shogun/shogun/neuralnet/training/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daichi/Workspace/oss/python3_ws/shogun/shogun/neuralnet/training/back_propagation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSquaredError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLossFunctionFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# Authors: Daichi Yoshikawa <daichi.yoshikawa@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .utils.nn_utils import get_kwarg, shuffle_data, split_data\n",
    "from .training.random_weight import RandomWeight\n",
    "from .training.back_propagation import BackPropagation\n",
    "from .layers.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Interface of neural network.\n",
    "\n",
    "    Training of model and prediction with resulting model\n",
    "    is done through this class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : np.array of derived class of Layer\n",
    "        Layers to build neural network.\n",
    "        The first layer must be InputLayer and last layer must be OutputLayer.\n",
    "    dtype : type\n",
    "        Data type selected through constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dtype : type, default np.float32\n",
    "            Data type to use.\n",
    "        \"\"\"\n",
    "        self.layers = np.array([], dtype=Layer)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"Add instance of derived class of layer.\n",
    "\n",
    "        Build neural network by adding layers one by one with this method.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        layer : Derived class of Layer\n",
    "            Instance of derived class of Layer.\n",
    "        \"\"\"\n",
    "        layer.set_dtype(self.dtype)\n",
    "        self.layers = np.append(self.layers, layer)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Finalize configuration of neural network model.\n",
    "\n",
    "        Warning\n",
    "        -------\n",
    "        This method must be called after adding all required layers\n",
    "        and before starting training.\n",
    "        \"\"\"\n",
    "        if self.layers.size == 0:\n",
    "            msg = 'NeuralNetwork has no layer.\\n'\\\n",
    "                + 'Please add layers before compiling.'\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        parent = self.layers[0]\n",
    "\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            layer.set_parent(parent)\n",
    "            parent = layer\n",
    "\n",
    "    def fit(self, x, y, optimizer, loss_function, **kwargs):\n",
    "        \"\"\"Train model.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : np.array\n",
    "            Descriptive features in 2d array,\n",
    "            whose shape is (num of data, num of feature)\n",
    "        y : np.array\n",
    "            Target features in 2d array,\n",
    "            whose shape is (num of data, num of feature)\n",
    "        optimizer : Derived class of Optimizer\n",
    "            Instance of derived class of Optimizer.\n",
    "        loss_function : LossFunction.Type\n",
    "            Type of loss function to use.\n",
    "        epochs : int, default 10\n",
    "            Number of iterations of training.\n",
    "            1 iteration scans all batches one time.\n",
    "        batch_size : int, default 100\n",
    "            Dataset is splitted into multiple mini batches\n",
    "            whose size is this.\n",
    "        monitor : bool, default True\n",
    "            Print out evaluation results of ongoing training.\n",
    "        shuffle : bool, default True\n",
    "            Shuffle dataset one time before training.\n",
    "        shuffle_per_epoch : bool, default False\n",
    "            Shuffle training data every epoch.\n",
    "        test_data_ratio : float, default 0\n",
    "            Ratio of test data. If 0, all data is used for training.\n",
    "\n",
    "        Warning\n",
    "        -------\n",
    "        This method assumes that x and y include all data you use.\n",
    "        If your data set is so large that all data cannot be stored in memory,\n",
    "        you cannot use this method. Use fit_gen instead.\n",
    "        \"\"\"\n",
    "        epochs = get_kwarg(\n",
    "                key='epochs',\n",
    "                dtype=int,\n",
    "                default_value=10,\n",
    "                **kwargs)\n",
    "        batch_size = get_kwarg(\n",
    "                key='batch_size',\n",
    "                dtype=int,\n",
    "                default_value=100,\n",
    "                **kwargs)\n",
    "        monitor = get_kwarg(\n",
    "                key='monitor',\n",
    "                dtype=bool,\n",
    "                default_value=True,\n",
    "                **kwargs)\n",
    "        shuffle = get_kwarg(\n",
    "                key='shuffle',\n",
    "                dtype=bool,\n",
    "                default_value=True,\n",
    "                **kwargs)\n",
    "        shuffle_per_epoch = get_kwarg(\n",
    "                key='shuffle_per_epoch',\n",
    "                dtype=bool,\n",
    "                default_value=False,\n",
    "                **kwargs)\n",
    "        test_data_ratio = get_kwarg(\n",
    "                key='test_data_ratio',\n",
    "                dtype=self.dtype,\n",
    "                default_value=self.dtype(0.),\n",
    "                **kwargs)\n",
    "\n",
    "        if shuffle:\n",
    "            x, y = shuffle_data(x, y)\n",
    "        x, y = self.__convert_dtype(x, y)\n",
    "        x_train, y_train, x_test, y_test = split_data(x, y, test_data_ratio)\n",
    "\n",
    "        back_prop = BackPropagation(epochs, batch_size, optimizer, loss_function, monitor, self.dtype)\n",
    "        back_prop.fit(self.layers, x_train, y_train, x_test, y_test, shuffle_per_epoch)\n",
    "\n",
    "    def fit_gen(self, x, y, optimizer, loss_function, **kwargs):\n",
    "        \"\"\"Train model for large size data set by using generator.\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        raise NotImplementError('NeuralNetwork.fit_one_batch')\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Returns predicted result.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : np.array\n",
    "            Discriptive features in 2d array,\n",
    "            whose shape is (num of data, num of features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Predicted target features in 2d array,\n",
    "            whose shape is (num of data, num of features)\n",
    "        \"\"\"\n",
    "        return self.layers[0].predict(x.astype(self.dtype))\n",
    "\n",
    "    def print_config(self):\n",
    "        \"\"\"Display configuration of layers.\"\"\"\n",
    "        i = 1\n",
    "        layer = self.layers[0]\n",
    "\n",
    "        while layer is not None:\n",
    "            print(str(i) + '-th layer : ' + layer.get_type())\n",
    "            layer = layer.child\n",
    "            i += 1\n",
    "\n",
    "    def __convert_dtype(self, x, y):\n",
    "        \"\"\"Convert data type of features into selected one in constructor.\"\"\"\n",
    "        return x.astype(self.dtype), y.astype(self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
