{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e636118c53d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBatchNormLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Implementation of Batch Normalization.\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mDerived\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mof\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mIt\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mcalculate\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Authors: Daichi Yoshikawa <daichi.yoshikawa@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .layer import Layer\n",
    "from ..utils import is_multi_channels_image\n",
    "from ..utils import flatten, unflatten\n",
    "\n",
    "class BatchNormLayer(Layer):\n",
    "    \"\"\"Implementation of Batch Normalization.\n",
    "\n",
    "    Derived class of Layer.\n",
    "    It doesn't calculate average and variance of all training data.\n",
    "    Instead, it approximately get them through\n",
    "    lowpass filter while training model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : np.array\n",
    "        1d matrix which is used to scale the normalized value.\n",
    "        (x'' = gamma * x' + beta)\n",
    "        This value is updated while training\n",
    "    beta : np.array\n",
    "        1d matrix which is used to shift the normalized value.\n",
    "        (x'' = gamma * x' + beta)\n",
    "        This value is updated while training.\n",
    "    miu : np.array\n",
    "        1d matrix which is composed of feature-wise means.\n",
    "    var : np.array\n",
    "        1d matrix which is composed of feature-wise variances.\n",
    "    momentum : float, default 0.9\n",
    "        Intensity of lowpass filter used to calculate average miu and var.\n",
    "    ep : float, default 1e-5\n",
    "        Used to avoid 0 division.\n",
    "    dtype : type\n",
    "        Data type of all numeric values.\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Batch Normalization: Accelerating Deep Network Training\n",
    "    by Reducing Internal Covariate Shift\n",
    "    http://proceedings.mlr.press/v37/ioffe15.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, momentum=0.9):\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.miu = None\n",
    "        self.var = None\n",
    "        self.momentum = momentum\n",
    "        self.ep = 1e-5\n",
    "\n",
    "    def set_dtype(self, dtype):\n",
    "        \"\"\"Set data type to use.\n",
    "\n",
    "        Warning\n",
    "        -------\n",
    "        Not supposed to be called directly.\n",
    "        Called automatically in the phase of NeuralNetwork's initialization.\n",
    "        \"\"\"\n",
    "        self.dtype = dtype\n",
    "        self.momentum = dtype(self.momentum)\n",
    "        self.ep = dtype(self.ep)\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'batch_norm'\n",
    "\n",
    "    def set_parent(self, parent):\n",
    "        Layer.set_parent(self, parent)\n",
    "        self.output_shape = self.input_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        miu = np.mean(x, axis=0)\n",
    "        self.xmiu = x - miu\n",
    "\n",
    "        var = np.mean(self.xmiu**2, axis=0)\n",
    "        self.std_inv = 1. / (np.sqrt(var + self.ep))\n",
    "\n",
    "        if self.gamma is None:\n",
    "            self.gamma = np.ones(self.input_shape, dtype=self.dtype)\n",
    "        if self.beta is None:\n",
    "            self.beta = np.zeros(self.input_shape, dtype=self.dtype)\n",
    "\n",
    "        self.xhat = self.xmiu * self.std_inv\n",
    "        self.fire = self.gamma * self.xhat + self.beta\n",
    "\n",
    "        if self.miu is None:\n",
    "            self.miu = miu\n",
    "        if self.var is None:\n",
    "            self.var = var\n",
    "\n",
    "        self.miu *= self.momentum\n",
    "        self.miu += (1. - self.momentum) * miu\n",
    "        self.var *= self.momentum\n",
    "        self.var += (1. - self.momentum) * var\n",
    "\n",
    "        self.child.forward(self.fire)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        batch_size = dy.shape[0]\n",
    "\n",
    "        dbeta = dy.sum(axis=0)\n",
    "        dgamma = (self.xhat * dy).sum(axis=0)\n",
    "\n",
    "        tmp1 = (self.gamma * self.xmiu * dy).sum(axis=0)\n",
    "        tmp2 = -np.power(self.std_inv, 3) * tmp1 / batch_size\n",
    "        tmp3 = self.xmiu * tmp2 + self.gamma * self.std_inv * dy\n",
    "        tmp4 = tmp3.sum(axis=0)\n",
    "\n",
    "        self.backfire = tmp3 - tmp4 / batch_size\n",
    "        self.beta -= dbeta / batch_size\n",
    "        self.gamma -= dgamma / batch_size\n",
    "\n",
    "        self.parent.backward(self.backfire)\n",
    "\n",
    "    def predict_to_eval(self, x):\n",
    "        self.fire = self.gamma * (x - self.miu) / np.sqrt(self.var + self.ep) + self.beta\n",
    "        return self.child.predict_to_eval(self.fire)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.fire = self.gamma * (x - self.miu) / np.sqrt(self.var + self.ep) + self.beta\n",
    "        return self.child.predict(self.fire)\n",
    "\n",
    "    def finalize_training(self, x):\n",
    "        self.fire = self.gamma * (x - self.miu) / np.sqrt(self.var + self.ep) + self.beta\n",
    "        self.child.finalize_training(self.fire)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
