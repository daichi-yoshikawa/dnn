{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Daichi Yoshikawa <daichi.yoshikawa@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "from .layer import Layer\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    \"\"\"Implements layer which convert values by activation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : Derived class of Activation\n",
    "        Activation function to use.\n",
    "    shape : tuple\n",
    "        Shape of this layer's neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        activation : Activation.Type\n",
    "           Name of activation function to use.\n",
    "        \"\"\"\n",
    "        self.activation = ActivationFactory.get(activation)\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'activation'\n",
    "\n",
    "    def set_parent(self, parent):\n",
    "        Layer.set_parent(self, parent)\n",
    "        self.shape = parent.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.fire = self.activation.activate(x)\n",
    "        self.child.forward(self.fire)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        self.backfire = dy * self.activation.grad(self.fire)\n",
    "        self.parent.backward(self.backfire)\n",
    "\n",
    "    def predict_to_eval(self, x):\n",
    "        self.fire = self.activation.activate(x)\n",
    "        return self.child.predict_to_eval(self.fire)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.fire = self.activation.activate(x)\n",
    "        return self.child.predict(self.fire)\n",
    "\n",
    "    def finalize_training(self, x):\n",
    "        self.fire = self.activation.activate(x)\n",
    "        self.child.finalize_training(self.fire)\n",
    "\n",
    "# Implement Activation Functions\n",
    "class Activation:\n",
    "    \"\"\"Base class for activation functions.\n",
    "\n",
    "    Warning\n",
    "    -------\n",
    "    This class should not be used directly.\n",
    "    Use derived classes instead. \n",
    "    \"\"\"\n",
    "    Type = Enum('Type', 'sigmoid, relu, elu, srrelu, tanh, softmax')\n",
    "\n",
    "    def get_type(self):\n",
    "        \"\"\"Interface to get type of activation in string.\"\"\"\n",
    "        raise NotImplementedError('Activation.get_type')\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Interface to get f(x), where f(x) is activation function.\"\"\"\n",
    "        raise NotImplementedError('Activation.activate')\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"Interface to get df(x)/dx, where f(x) is activation function.\"\"\"\n",
    "        raise NotImplementedError('Activation.grad')\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"Sigmoid function.\n",
    "\n",
    "    f(x) = 1 / (1 + exp(-a*x)).\n",
    "    df(x)/dx = (1 - f(x)) * f(x)\n",
    "    This class implements aboves with a == 1.\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'sigmoid'\n",
    "\n",
    "    def activate(self, x):\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def grad(self, x):\n",
    "        return (1. - x) * x\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"Rectified linear function.\n",
    "\n",
    "    f(x) = x when x > 0 and f(x) = 0 when x <= 0.\n",
    "    df(x)/dx = 1 when x > 0 and df(x)/dx = 0 when x <= 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    __mask : np.array\n",
    "        2d matrix whose entry(i, j) is \n",
    "        1 when x(i, j) > 0 and 0 when x(i, j) == 0.\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'relu'\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.__mask = (x > 0.).astype(x.dtype)\n",
    "        return x * self.__mask\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.__mask\n",
    "\n",
    "\n",
    "class ELU(Activation):\n",
    "    \"\"\"Exponential Linear Units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, default 1.0\n",
    "        Controls the value to which an ELU saturates for negative net inputs.\n",
    "    __mask : np.array\n",
    "        2d matrix whose entry(i, j) is \n",
    "        1 when x(i, j) > 0 and 0 when x(i, j) == 0.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n",
    "    https://arxiv.org/pdf/1511.07289.pdf\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'elu'\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.__mask = (x > 0.)\n",
    "        return self.__mask * x + ~(self.__mask) * (np.exp(x) - 1)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.__mask + ~(self.__mask) * (x + 1)\n",
    "\n",
    "\n",
    "class SRReLU(Activation):\n",
    "    \"\"\"Square Root version of Rectified Linear.\n",
    "\n",
    "    This is an activation function the author, Daichi Yoshikawa, proposes.\n",
    "    To reduce bias shift and enhance nonlinearity, use square root of input\n",
    "    instead of original value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    __mask : np.array\n",
    "        2d matrix whose entry(i, j) is \n",
    "        1 when x(i, j) > 0 and 0 when x(i, j) == 0.\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'selu'\n",
    "\n",
    "    def activate(self, x):\n",
    "        self.__mask = (x > 0.)\n",
    "        return (np.sqrt(self.__mask * x + 1) - 1)# + ~self.__mask * self.alpha * (np.exp(x) - 1)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.__mask * 0.5 / (x + 1)# + ~self.__mask * (x + self.alpha)\n",
    "\n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\"Tanh function.\n",
    "\n",
    "    f(x) = tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "    df(x)/dx = 1 - f(x)^2\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'tanh'\n",
    "\n",
    "    def activate(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return 1. - np.power(x, 2, dtype=x.dtype)\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"Softmax function to convert values into probabilities.\n",
    "\n",
    "    f(x) = exp(x) / exp(x).sum()\n",
    "         = exp(x) * exp(-c) / (exp(x).sum()) * exp(-c)\n",
    "         = exp(x - c) / exp(x - c).sum()\n",
    "    where c == x.max(), to avoid overflow of exp calculation.\n",
    "    \"\"\"\n",
    "    def get_type(self):\n",
    "        return 'softmax'\n",
    "\n",
    "    def activate(self, x):\n",
    "        var = np.exp(x - x.max())\n",
    "        return var / var.sum(axis=1).reshape(var.shape[0], 1)\n",
    "\n",
    "    def grad(self, x):\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class ActivationFactory:\n",
    "    \"\"\"Factory class to get activation's instance.\n",
    "\n",
    "    Warning\n",
    "    -------\n",
    "    Get activation's instance through this class.\n",
    "    \"\"\"\n",
    "    __activation = {\n",
    "            Activation.Type.sigmoid : Sigmoid(),\n",
    "            Activation.Type.relu : ReLU(),\n",
    "            Activation.Type.elu : ELU(),\n",
    "            Activation.Type.srrelu : SRReLU(),\n",
    "            Activation.Type.tanh : Tanh(),\n",
    "            Activation.Type.softmax : Softmax(),\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, activation):\n",
    "        \"\"\"Returns instance of selected activation function.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        activation : Activation.Type\n",
    "            Name of activation function to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Derived class of Activation\n",
    "            Instance of selected activation function.\n",
    "        \"\"\"\n",
    "        return cls.__activation[activation]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
