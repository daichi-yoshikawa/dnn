{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Authors: Daichi Yoshikawa <daichi.yoshikawa@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import dnn\n",
    "from dnn.neuralnet import NeuralNetwork\n",
    "from dnn.utils.nn_utils import scale_normalization\n",
    "\n",
    "from dnn.training.optimizer import Adam, AdaGrad, AdaDelta, Momentum\n",
    "from dnn.training.random_weight import RandomWeight\n",
    "from dnn.training.loss_function import LossFunction\n",
    "\n",
    "from dnn.layers.layer import InputLayer, OutputLayer\n",
    "from dnn.layers.affine import AffineLayer\n",
    "\n",
    "from dnn.layers.activation import Activation, ActivationLayer\n",
    "from dnn.layers.dropout import DropoutLayer\n",
    "from dnn.layers.batch_norm import BatchNormLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    x = np.load('input1.npy')\n",
    "    x = np.r_[x, np.load('input2.npy')]\n",
    "    x = np.r_[x, np.load('input3.npy')]\n",
    "    x = np.r_[x, np.load('input4.npy')]\n",
    "    x = np.r_[x, np.load('input5.npy')]\n",
    "    x = np.r_[x, np.load('input6.npy')]\n",
    "    x = np.r_[x, np.load('input7.npy')]\n",
    "    x = x.astype(float)\n",
    "    \n",
    "    y = np.load('output.npy')\n",
    "    y = y.astype(float)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    1, loss: 0.110394, acc: 0.968,                  test loss: 0.133125, test acc: 0.960\n",
      "epoch:    2, loss: 0.078277, acc: 0.977,                  test loss: 0.106670, test acc: 0.968\n",
      "epoch:    3, loss: 0.063568, acc: 0.981,                  test loss: 0.095058, test acc: 0.972\n",
      "epoch:    4, loss: 0.052897, acc: 0.984,                  test loss: 0.086057, test acc: 0.973\n",
      "epoch:    5, loss: 0.046851, acc: 0.986,                  test loss: 0.081787, test acc: 0.975\n",
      "epoch:    6, loss: 0.040651, acc: 0.988,                  test loss: 0.077779, test acc: 0.977\n",
      "epoch:    7, loss: 0.035288, acc: 0.989,                  test loss: 0.070878, test acc: 0.979\n",
      "epoch:    8, loss: 0.034587, acc: 0.990,                  test loss: 0.071929, test acc: 0.977\n",
      "epoch:    9, loss: 0.030291, acc: 0.991,                  test loss: 0.067616, test acc: 0.979\n",
      "epoch:   10, loss: 0.030361, acc: 0.991,                  test loss: 0.071614, test acc: 0.977\n",
      "epoch:   11, loss: 0.026260, acc: 0.992,                  test loss: 0.066990, test acc: 0.979\n",
      "epoch:   12, loss: 0.023940, acc: 0.993,                  test loss: 0.063788, test acc: 0.980\n",
      "epoch:   13, loss: 0.022392, acc: 0.994,                  test loss: 0.063229, test acc: 0.980\n",
      "epoch:   14, loss: 0.021491, acc: 0.994,                  test loss: 0.063096, test acc: 0.980\n",
      "epoch:   15, loss: 0.020502, acc: 0.994,                  test loss: 0.063546, test acc: 0.981\n",
      "epoch:   16, loss: 0.020224, acc: 0.994,                  test loss: 0.063823, test acc: 0.980\n",
      "epoch:   17, loss: 0.019322, acc: 0.994,                  test loss: 0.062220, test acc: 0.981\n",
      "epoch:   18, loss: 0.016842, acc: 0.995,                  test loss: 0.063059, test acc: 0.981\n",
      "epoch:   19, loss: 0.016566, acc: 0.995,                  test loss: 0.063313, test acc: 0.980\n",
      "epoch:   20, loss: 0.015925, acc: 0.996,                  test loss: 0.060847, test acc: 0.981\n",
      "0.995533333333\n",
      "0.9807\n"
     ]
    }
   ],
   "source": [
    "dtype = np.float32\n",
    "neuralnet = NeuralNetwork(dtype=dtype)\n",
    "neuralnet.add(InputLayer(shape=784))\n",
    "neuralnet.add(DropoutLayer(drop_ratio=0.2))\n",
    "neuralnet.add(AffineLayer(shape=(784, 400), random_weight=RandomWeight.Type.he))\n",
    "neuralnet.add(BatchNormLayer())\n",
    "neuralnet.add(ActivationLayer(activation=Activation.Type.relu))\n",
    "neuralnet.add(DropoutLayer(drop_ratio=0.5))\n",
    "neuralnet.add(AffineLayer(shape=(400, 10), random_weight=RandomWeight.Type.xavier))\n",
    "neuralnet.add(BatchNormLayer())\n",
    "neuralnet.add(ActivationLayer(activation=Activation.Type.softmax))\n",
    "neuralnet.add(OutputLayer(shape=10))\n",
    "neuralnet.compile()\n",
    "\n",
    "x, y = get_mnist()\n",
    "scale_normalization(x)\n",
    "\n",
    "optimizer = AdaGrad(learning_rate=3e-2, weight_decay=1e-3, dtype=dtype)\n",
    "\n",
    "neuralnet.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=20,\n",
    "        batch_size=100,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=LossFunction.Type.multinomial_cross_entropy,\n",
    "        monitor=True,\n",
    "        shuffle=True,\n",
    "        shuffle_per_epoch=True,\n",
    "        test_data_ratio=0.142857\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
